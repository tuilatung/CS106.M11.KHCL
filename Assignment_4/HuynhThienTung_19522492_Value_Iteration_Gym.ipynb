{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuynhThienTung_19522492_Value_Iteration_Gym.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtRrVWU-Wm4F"
      },
      "source": [
        "Họ và tên: Huỳnh Thiện Tùng\n",
        "\n",
        "MSSV: 19522492\n",
        "\n",
        "Bài tập: LAB 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbNbFDXWyns"
      },
      "source": [
        "# Import các thư viện cần thiết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EfdvH5hSDXm"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyhFwGpY9t0X"
      },
      "source": [
        "#### Khởi tạo môi trường FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRO2o9axS4bS"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7x8blfWUtgJ",
        "outputId": "5e20a099-b50f-4791-b09e-fc014851e444"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQwOVrHSXKeM"
      },
      "source": [
        "Không gian trạng thái quan sát: 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp56rqQ6VVU-",
        "outputId": "0ab454ce-9bac-4bbc-de06-6c83351787a6"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3o8HJsdXPZk"
      },
      "source": [
        "Không gian hành động: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urV-N8QSVZm2",
        "outputId": "c481cde3-28a5-4b47-b744-9099a0d75a94"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoqZucGtKFc7"
      },
      "source": [
        "#### Cài đặt thuật toán Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-llzIO6xU9bm"
      },
      "source": [
        "def value_iteration(env, max_iters, gamma):\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Calculate value of state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            # Calculate q-value for each action\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # Get the best action\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # If convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68jaXmJXTvR",
        "outputId": "4d0066c9-8deb-4205-9359-e696bdf6e4f2"
      },
      "source": [
        "v_values = value_iteration(env, max_iters=1000, gamma=0.9)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USWIyd1-XaAW",
        "outputId": "b2189dc9-85e5-4d73-ddb1-96e203f36052"
      },
      "source": [
        "v_values"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDZPTH3Xgm0"
      },
      "source": [
        "#### Cài đặt phương thức policy extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNbLFNwsXhNF"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # Calculate q_value for each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # Get the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufJ7WjSYGEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cece1e-a3bd-4fff-f8c4-5bf166d5af13"
      },
      "source": [
        "policy = policy_extraction(env, v_values, gamma=0.9)\n",
        "policy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSoh5qJVKXn2"
      },
      "source": [
        "#### Cài đặt thuật toán Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjMmepNPvkXO"
      },
      "source": [
        "def policy_iteration(env, max_iters, gamma):\n",
        "\n",
        "    # Initialization\n",
        "    ini_pi = np.array([env.action_space.sample() for i in range(env.observation_space.n)])\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        # Policy Evaluation\n",
        "        v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "        for j in range(max_iters):\n",
        "            prev_v_values = np.copy(v_values)\n",
        "\n",
        "            # Calculate value of state\n",
        "            for state, action in enumerate(ini_pi):\n",
        "                # Calculate q-value for each action\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                v_values[state] = q_value\n",
        "          \n",
        "            # Check for convergence\n",
        "            if np.all(np.isclose(v_values, prev_v_values)):\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        prev_pi = np.copy(ini_pi)\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            # Calculate q-value for each action\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # Loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # Get the best action\n",
        "            best_action = np.argmax(q_values)\n",
        "            ini_pi[state] = best_action\n",
        "            \n",
        "        # If convergence\n",
        "        if np.all(np.isclose(ini_pi, prev_pi)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return ini_pi"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1aWPlmJ8OXL",
        "outputId": "84938613-f66e-4b37-fc27-01327d7a4761"
      },
      "source": [
        "pi = policy_iteration(env, max_iters=1000, gamma=0.9)\n",
        "pi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 3-th iteration.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cca5B02dZAl2"
      },
      "source": [
        "def play(env, policy):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    #time.sleep(1)\n",
        "    \n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        state = next_state\n",
        "\n",
        "    return total_reward"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w46kFNsGZ2o1",
        "outputId": "a755fc37-aaf6-4123-9134-1c529f38cd68"
      },
      "source": [
        "play(env, policy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nrMm7uyZ5we"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "\n",
        "    for i in range(max_episodes):\n",
        "        reward = play(env, policy)\n",
        "\n",
        "        if reward > 0:\n",
        "            success += 1\n",
        "    \n",
        "    return success"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3urMjXOyYJi8"
      },
      "source": [
        "Cho chạy 1000 lần thì có khoảng 743 lần thành công, tuy nhiên chúng ta cần con số chính xác hơn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4K4We01a6gm",
        "outputId": "82b5b639-2f8e-4134-d0a4-2ea3356643b5"
      },
      "source": [
        "play_multiple_times(env, policy, 1000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "743"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdaDF6ThYdm8"
      },
      "source": [
        "Cho chạy 2000 episodes, mỗi eposodes chạy 1000 vòng lặp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbX-rxo28831"
      },
      "source": [
        "# Initilize parameters\n",
        "MAX_ITERS = 1000\n",
        "MAX_EPISODES = 2000\n",
        "GAMMA = 0.9"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bku4DO9LXVW"
      },
      "source": [
        "#### Thực nghiệm trên môi trường FrozenLake-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxNWZfsqRS2W",
        "outputId": "251ac2f3-f0db-4e6c-b857-b1f997bc582f"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time : {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n",
            "Converged at 5-th iteration.\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 1453/2000, Average time : 0.0007138723134994507s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 1443/2000, Average time : 0.0006920249462127686s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqFseWrlLdIr"
      },
      "source": [
        "#### Thực nghiệm trên FrozenLake8x8-v0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImTSnepWTM4M",
        "outputId": "c286d4bd-ae78-40aa-ea94-001cf37962dd"
      },
      "source": [
        "env = gym.make('FrozenLake8x8-v0')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time :  {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 117-th iteration.\n",
            "Converged at 3-th iteration.\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 1470/2000, Average time :  0.0012719990015029907s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 1433/2000, Average time : 0.0013079802989959717s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb_4x0H5Lla1"
      },
      "source": [
        "#### Thực nghiệm trên Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKLyJTqXTSDm",
        "outputId": "d421ac70-70d4-4893-a544-50f376c29421"
      },
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "vi_value = value_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "policy_from_value = policy_extraction(env, vi_value, GAMMA)\n",
        "start_vi = time.time()\n",
        "vi_number_of_successes = play_multiple_times(env, policy=policy_from_value, max_episodes=MAX_EPISODES)\n",
        "vi_time = time.time() - start_vi\n",
        "\n",
        "pi = policy_iteration(env, max_iters=MAX_ITERS, gamma=GAMMA)\n",
        "start_pi = time.time()\n",
        "pi_number_of_successes = play_multiple_times(env, policy=pi, max_episodes=MAX_EPISODES)\n",
        "pi_time = time.time() - start_pi\n",
        "\n",
        "print(f'Number of successes of Value Iteration in FrozenLake8x8-v0 : {vi_number_of_successes}/{MAX_EPISODES}, Average time : {vi_time/MAX_EPISODES}s')\n",
        "print(f'Number of successes of Policy Iteration in FrozenLake8x8-v0 : {pi_number_of_successes}/{MAX_EPISODES}, Average time : {pi_time/MAX_EPISODES}s')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 116-th iteration.\n",
            "Converged at 17-th iteration.\n",
            "Number of successes of Value Iteration in FrozenLake8x8-v0 : 2000/2000, Average time : 0.0003414106369018555s\n",
            "Number of successes of Policy Iteration in FrozenLake8x8-v0 : 2000/2000, Average time : 0.0003098808526992798s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YB5FsHn9bzD"
      },
      "source": [
        "#### Nhận xét"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-wpuboi9hKw"
      },
      "source": [
        "Từ kết quả chạy thục nghiệm ta có thể thấy rằng kết quả về số ván thắng lẫn score của 3 map trên với 2 loại giải thuật Value Iteration và Policy Iteration là có sự chênh lệch khá rõ rệt. Giải thuật Policy Iteration hội tụ nhanh hơn so với giải thuật Value Iteration và thời gian chạy cũng nhanh hơn tuy nhiên sự chênh lệch không lớn lắm đối với 2000 EPISODES. \n",
        "\n",
        "Xét về độ góc độ cài đặt, Policy Iteration nhìn chung có vẻ phức tạp hơn nhưng chạy nhanh hơn so với Value Iteration. Cả hai giải thuật này đều bảo đảm rằng sẽ hội tụ tới một chiến lược tối ưu, tuy nhiên hai giải thuật này có một số đặc điểm khác biệt rõ về mặc cài đặc giải thuật, chi phí tính toán, tốc độ thực thi,..."
      ]
    }
  ]
}